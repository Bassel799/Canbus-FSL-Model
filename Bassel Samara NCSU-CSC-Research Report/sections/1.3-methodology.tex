\subsection{Methodology}
\label{subsec:methodology}

\section{Methods and Materials}
\subsection{Data Preparation}
Datasets \texttt{SynCanAttacks} and \texttt{CarHackingAttacks}, which included actual CAN Bus attack scenarios arranged by attack type, were used in the study. The SynCanAttacks dataset yielded 13 features with 4 signals per message, while the Car-Hacking dataset \cite{seo2018carhacking} yielded 25 features with 8 signals per message. The datasets included 545 and 1200 attack blocks, respectively.

\subsection{Feature Extraction and Tensor Transformation}
In order to transform unstructured CAN bus messages into structured tensor representations appropriate for few-shot learning, the CANBus FSL Classifier put into practice a rigorous feature extraction pipeline. The system computed three statistical measures per signal—standard deviation (stdev), mean, and median—for every attack block found during data segmentation. These metrics were chosen because they were able to capture the central tendencies and volatility of attack patterns, which were important for differentiating between malicious and benign message sequences \cite{deng2013time}. To provide more contextual information, a message count feature was added to measure the amount of traffic during each attack block.

With datasets that had $N$ signals per message (where $N$ = 4 for SynCanAttacks and $N$ = 8 for Car-Hacking), this procedure produced $3N + 1$ total features per block. Following that, the transformation pipeline converted these features into a tensor format that convolutional neural networks could use. The features were first dynamically rearranged into $N \times 3 \times 1$ matrices, with each row denoting a signal and the corresponding stdev, mean, and median values in the columns. To ensure consistent gradient behavior during backpropagation, these matrices were subjected to MinMax normalization, which scaled all values to the interval [0, 1]. The normalized tensors were replicated along the channel dimension to create final tensors of size $3 \times N \times 3$, which simulated RGB-like input channels anticipated by conventional CNN architectures. Despite being straightforward, this replication technique successfully enabled the model to use pretrained convolutional weights without requiring any changes to the architecture. The \texttt{CANBusFewShotDataset} class contained the entire pipeline, automating batch generation and guaranteeing compatibility with PyTorch's DataLoader tools.

\subsection{Model Architecture}
Initially, features were dynamically rearranged into $N \times 3 \times 1$ matrices, with each row denoting a signal and each column holding the corresponding median, mean, and standard deviation values. All of the values in these matrices were scaled to the interval [0, 1] using MinMax normalization, which guaranteed uniform gradient behavior throughout backpropagation. The final tensors were of size $3 \times N \times 3$ after the normalized tensors were replicated along the channel dimension to mimic RGB-like input channels that are expected by conventional CNN architectures. Even though this replication technique was straightforward, it successfully enabled the model to use pretrained convolutional weights without requiring changes to the architecture. The ResNet12 backbone, a compact residual network derived from the original ResNet architecture, served as the foundation for the convolutional encoder used in all experiments \cite{he2016resnet}. The entire pipeline was split into a 80/20 ratio with 80\% of the data being used to train the model, and 20\% being used to test, resulting in the Syncan and Carhacking datasets to include 436 and 960 attack blocks, respectively the other contained in the \texttt{CANBusFewShotDataset} class, which guaranteed compatibility with PyTorch's DataLoader tools and automated batch generation.

A crucial aspect of the implementation was the use of 4-way classification tasks in training, with five support examples and ten query examples per class per episode. The limitations in the real world, where new threat categories may have limited access to labeled attack data, were reflected in this setup. The model's input pipeline was designed to handle variable signal counts ($N$) using dynamic tensor reshaping, resulting in consistent performance across datasets with different message structures.
